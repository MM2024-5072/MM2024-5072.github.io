<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhancing Audio-Visual Emotion Recognition by Leveraging Unimodal and Multimodal Perceived Emotions</title>
    <!-- Bootstrap CSS -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
	<style type="text/css">
		  table {
			width: 100%;
			table-layout: fixed;
		  }

		  audio {
			width: 100%;
		  }

		  thead>tr>th:first-child {
			width: 96px;
		  }
		td{
			text-align: center;
		}
			
		  @media (max-width: 767px) {
			.big-screen {
			  display: none;
			}
		  }

		  @media (min-width: 767px) {
			.small-screen {
			  display: none;
			}
		  }
		.center {
		  display: block;
		  margin-left: auto;
		  margin-right: auto;
		}

		
	</style>
</head>
<body>
    
	<div class="jumbotron bg-secondary text-center">
	  <div class="container">
		<div class="row align-items-center">
		  <div class="col-md-12">
			 <h1><a class="text-light"> Enhancing Audio-Visual Emotion Recognition by Leveraging Unimodal and Multimodal Perceived Emotions</a> </h1>
		  </div>
		</div>
	  </div>
	</div>
	
	<div class="container">

		<img src="./data/teaser.png"  class="center" width="100%" / >
		<p class="lead">Audio-visual emotion recognition (AVER) has been one of the critical technologies in human-centered computing, modeling emotional signals from audio-visual data. Traditionally, audio-visual emotional datasets and corresponding models derive their ground truths from annotations attributed to the complete audio-visual spectrum, serving as the stimulus for data annotation. This conventional method, however, neglects the nuanced human perception of emotional states, which varies when annotations are made under different emotion stimuli conditionsâ€”whether through unimodal or multimodal stimuli.</p>

        <p>This study investigates the potential for enhanced AVER system performance by integrating diverse levels of annotation stimuli, reflective of varying perceptual evaluations. We propose a two-stage training method to train models with labels elicited by audio-only, face-only, and audio-visual stimulus. In our approach, we utilize different levels of annotation stimuli according to which modality is present within different layers of the model, effectively modeling annotation at the unimodal and multi-modal levels to capture the full scope of emotion perception across unimodal and multimodal contexts. We conduct and evaluate the CREMA-D emotion database, and the proposed methods achieved the best performances in macro-/weighted-F1 scores.</p>

        <p>Additionally, we measure the model calibration, performance bias, and fairness considering age, gender, and race of the AVER systems. Further analyses show that the proposed method can increase not only recognition rates but also model calibration and fairness based on age groups. To test the reliability of the proposed method, we simulate the missing modality scenarios by masking the random frames of inputs. Lastly, to test the reliability of the proposed method, we simulate the missing modality scenarios by masking the random frames of inputs.</p>
    </div>

    <!-- Bootstrap JS and dependencies -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.2/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
